"""
cleaner.py

Data cleaning utilities for preprocessing a pandas DataFrame.

Includes:
- Column name standardization
- Duplicate row removal
- Type coercion (booleans, IDs, numerics, dates)
- Missing value handling (dropping or imputation)
"""

import pandas as pd
from .log_setup.setup import setup, logging
import re
import pandas.api.types as pd_types


logger = logging.getLogger(__name__)
setup(logger)


def clean_pipeline(df: pd.DataFrame) -> pd.DataFrame:
    """
    Main orchestration function for the cleaning pipeline.

    Applies the following steps in order:
    1. Standardize column names (e.g., lowercase, snake_case)
    2. Remove exact duplicate rows
    3. Coerce nullable data types on columns
    4. Coerce eda types data types on columns
    5. Handle missing values (drop columns with >50% missing, impute others)

    Parameters:
        df (pd.DataFrame): The input DataFrame to be cleaned.

    Returns:
        pd.DataFrame: The cleaned DataFrame.
    """
    print("*" * 90)
    df = standardize_column_names(df)
    print("*" * 90)
    df = remove_duplicates(df)
    print("*" * 90)
    df = coerce_nullable_data_types(df)
    print("*" * 90)
    df = coerce_eda_types(df)
    print("*" * 90)
    df = handle_missing_values(df)
    return df


def standardize_column_names(df: pd.DataFrame) -> pd.DataFrame:
    """
    Standardizes DataFrame column names by:
    - Removing leading/trailing whitespace
    - Converting to lowercase
    - Replacing spaces/hyphens with underscores
    - Removing all non-alphanumeric characters (except underscores)

    Also removes a leading 'Unnamed' column if generated by Pandas.

    Parameters:
        df (pd.DataFrame): The input DataFrame.

    Returns:
        pd.DataFrame: DataFrame with standardized column names.
    """
    logger.info("Standardizing column names")
    print("*" * 90)
    if df.iloc[:, 0].name.startswith("Unnamed"):
        logger.info("Removing pandas index column")
        df = df.iloc[:, 1:]
        logger.info("Index column removed")

    columns_nr = df.shape[1]
    logger.info(f"Found {columns_nr} columns")
    logger.info(
        "replacing whitespaces with '_', lowering case, and removing invalid characters"
    )
    original_columns = df.columns.to_list()
    df.columns = (
        df.columns.str.strip()
        .str.lower()
        .str.replace(r"[ -]", "_", regex=True)
        .str.replace(r"[^\w]", "", regex=True)
    )
    for original_col, new_col in zip(
        original_columns, df.columns.to_list()
    ):
        if original_col == new_col:
            logger.info(f"{original_col} remained unchanged")
        else:
            logger.info(f"{original_col} changed to {new_col}")
    logger.info("Finished standardizing column names")
    return df


def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:
    """
    Function that removes duplicate rows, but also removes a row by following the procedure below:
    (Rest is deprecated)
    1) Identify 'id' column names (regex validation)
    2) Check for duplication, excluding the 'id' columns in the subset
    3) Remove duplication, keep in the first occurrence.
    In any case we are removing duplicates with the 'keep-first' strategy
    """
    logger.info("Removing duplicate rows")
    print("*" * 90)
    logger.info(f"{df.shape[0]} rows before operation")
    logger.info("Removing...")
    try:
        no_dup_df = df.drop_duplicates(keep="first")
    except TypeError:
        logger.error(
            "Dataset contains unhashable type columns, cannot remove rows based on full row duplication."
        )
        no_dup_df = df

    logger.info(f"{df.shape[0] - no_dup_df.shape[0]} rows removed")
    logger.info(f"{no_dup_df.shape[0]} rows remaining.")
    logger.info(f"Finished removing duplicate rows")
    return no_dup_df


# will rename to coerce nullable data types
# This one is done for compatibility
def coerce_nullable_data_types(df: pd.DataFrame) -> pd.DataFrame:
    """
    Processes column series, and applies casts them to the appropriate
    pandas nullable data type.
    Nullable data types are the ones that support pd.NA for missing values

    Parameters:
        df (pd.DataFrame): The source Dataframe

    Returns:
        df (pd.DataFrame): The source dataframe with updated
        column series
    """
    logger.info("Converting columns to nullable data types")
    print("*" * 90)
    nullable_df = pd.DataFrame()
    for col in df.columns:
        logger.info(f"Processing column {col}")
        series = df[col]
        non_null_series = series.dropna()

        if non_null_series.empty:
            # default to object if there's nothing to infer
            nullable_df[col] = series.astype("object")
            continue

        inferred_dtype = pd.api.types.infer_dtype(
            non_null_series, skipna=True
        )

        # Map inferred dtype to a pandas nullable type
        if inferred_dtype in {"integer"}:
            nullable_df[col] = series.astype("Int64")
            logger.info(f"Changed {col} to Int64")
        elif inferred_dtype in {"floating"}:
            try:
                nullable_df[col] = series.astype("Int64")
                logger.info(f"Changed {col} to Int64")
            except:
                nullable_df[col] = series.astype("Float64")
                logger.info(f"Changed {col} to Float64")
        elif inferred_dtype in {"boolean"}:
            nullable_df[col] = series.astype("boolean")
            logger.info(f"Changed {col} to boolean")
        elif inferred_dtype in {"string", "unicode", "datetime"}:
            try:
                nullable_df[col] = series.astype("datetime64[ns]")
                logger.info(f"Changed {col} to datetime64[ns]")
            except:
                nullable_df[col] = series.astype("string")
                logger.info(f"Changed {col} to string")
        else:
            nullable_df[col] = series.astype("object")
            logger.info(f"Changed {col} to object")

    logger.info("Finished converting columns to nullable data types")
    return nullable_df


def coerce_eda_types(df: pd.DataFrame) -> pd.DataFrame:
    """
    Processes column series, and  casts them to a type more suitable
    for eda. It 'captures' categorical, id and hidden boolean columns

    Parameters:
        df (pd.DataFrame): The source Dataframe, expects a nullable
        dtype (supporting pd.NA).

    Returns:
        df (pd.DataFrame): The source dataframe with updated
        column series.
    """
    logger.info("Converting columns to EDA-ready nullable types")
    print("*" * 90)
    for col in df.columns:
        logger.info(f"Processing column {col}")
        series = df[col]
        if _is_id_column(series):
            df[col] = series.astype("string")
            logger.info(f"Changed {col} from numeric to string")
        elif _is_binary_string(series):
            df[col] = _validate_binary_col(series)
        elif _is_numeric_boolean(series):
            df[col] = series.astype("boolean")
            logger.info(f"Changed {col} from numeric to boolean")
        elif _is_categorical(series):
            df[col] = series.astype("category")
            logger.info(f"Changed {col} to category data type")
    logger.info(
        "Finished converting columns to RDA-ready nullable types"
    )
    return df


def handle_missing_values(
    df: pd.DataFrame, drop_thres: float = 0.5
) -> pd.DataFrame:
    """
    Handles missing values in a DataFrame using a two-step strategy:

    1. If a column has a percentage of missing values >= `drop_thres` (default 50%),
       it is dropped entirely.
    2. Otherwise, missing values are imputed using `_impute()`, which:
        - Imputes numerics with the median (or another strategy)
        - Skips categorical and identifier-like columns

    Parameters:
        df (pd.DataFrame): The input DataFrame with potential missing values.
        drop_thres (float, optional): Threshold (as a proportion) for dropping a column
                                       based on missing value percentage. Defaults to 0.5.

    Returns:
        pd.DataFrame: The DataFrame with missing values either dropped or imputed.
    """
    logger.info("Handling missing Values")
    print("*" * 90)
    for column in df.columns:
        missing_values_prc = df[column].isnull().mean() * 100
        if missing_values_prc == 0:
            logger.info(
                f"{df[column].name} column has none of its values missing, skipping"
            )
            continue
        logger.info(
            f"{df[column].name} column has {round(missing_values_prc)}% of its values missing"
        )
        if missing_values_prc >= 50:
            logger.info(f"Dropping column {df[column].name}")
            df = df.drop(column, axis=1)
        else:
            df[column] = _impute(df[column])
    logger.info("Finished Handling Missing Values")
    return df


def _is_binary_string(col_series: pd.Series) -> bool:
    return (
        pd_types.is_string_dtype(col_series)
        and col_series.dropna().apply(str).str.lower().nunique() == 2
    )


def _is_numeric_boolean(col_series: pd.Series) -> bool:
    return pd_types.is_integer_dtype(col_series) and set(
        col_series.dropna().unique()
    ).issubset({0, 1})


def _is_id_column(col_series: pd.Series) -> bool:
    col_name = col_series.name
    return pd_types.is_numeric_dtype(col_series) and re.search(
        r"((?:(?<=_)|^)id(?=_))|.*id$", col_name, re.IGNORECASE
    )


def _impute(col_series: pd.Series, nmode="median") -> pd.Series:
    """
    Using nmode to impute the values of the provided column
    In case of invalid input the default nmode will be used.

    Note:
        Only addresses non-categorical, non-id columns
    """

    if (
        pd_types.is_numeric_dtype(col_series)
        # and not _is_categorical(col_series) # coerce eda types eliminates
        # and not _is_id_column(col_series)   # these two possibilities
    ):
        logger.info(
            f"Initiating imputation on {col_series.name} of dtype {col_series.dtype.name}"
        )
        col_series = col_series.astype("Float64")
        if nmode == "median":
            logger.info("Performing imputation with 'median'")
            col_series = col_series.fillna(col_series.median())
        elif nmode == "mean":
            logger.info("Performing imputation with 'mean'")
            col_series = col_series.fillna(col_series.mean())
        else:
            default_nmode = _impute.__defaults__[0]
            _impute(col_series, nmode=default_nmode)

        if (col_series.dropna() == col_series.astype(int)).all():
            col_series = col_series.astype("Int64")

    else:
        logger.info(
            f"{col_series.name} column, unsuitable for imputation, skipping."
        )

    return col_series


def _validate_binary_col(col_series: pd.Series) -> pd.Series:
    """
    Detect boolean-like columns and convert them to columns of bool dtype, otherwise convert them to string
    """
    # Check if all non-null rows are 'true' or 'false'
    if all(
        map(
            lambda x: (
                True
                if (
                    isinstance(x, type(pd.NA))
                    or str(x).lower() == "true"
                    or str(x).lower() == "false"
                )
                and not re.match(r"\s*$", str(x))
                else False
            ),
            col_series,
        )
    ):
        # if they are convert them to boolean
        col_series = col_series.map(_true_false_to_bool).astype(
            "boolean"
        )
        logger.info(f"Changed {col_series.name}'s dtype to boolean")
    # Check if all non-null rows are 'yes' or 'no'
    elif all(
        map(
            lambda x: (
                True
                if (
                    isinstance(x, type(pd.NA))
                    or str(x).lower() == "yes"
                    or str(x).lower() == "no"
                )
                and not re.match(r"\s*$", str(x))
                else False
            ),
            col_series,
        )
    ):
        # if they are convert them to boolean
        col_series = col_series.map(_yes_no_to_bool).astype("boolean")
        logger.info(f"Changed {col_series.name}'s dtype to boolean")
    else:
        col_series = col_series.astype("category")

    return col_series


def _is_categorical(col_series: pd.Series) -> bool:
    try:
        verdict = col_series.nunique(dropna=True) < 13
    except:
        verdict = False
    return verdict


def _yes_no_to_bool(x):
    "Converts a value to True if it is 'yes', False otherwise"
    if isinstance(x, type(pd.NA)):
        return x
    else:
        func = lambda x: True if x.lower() == "yes" else False
        return func(x)


def _true_false_to_bool(x):
    "Converts a value to True if it is 'true', False otherwise"
    if isinstance(x, type(pd.NA)):
        return pd.NA
    else:
        func = lambda x: True if x.lower() == "true" else False
        return func(x)
