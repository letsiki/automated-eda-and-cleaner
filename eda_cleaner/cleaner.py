"""
cleaner.py

Data cleaning utilities for preprocessing a pandas DataFrame.

Includes:
- Column name standardization
- Duplicate row removal
- Type coercion (booleans, IDs, numerics, dates)
- Missing value handling (dropping or imputation)
"""

import pandas as pd
from .log_setup.setup import setup, logging
import re
import pandas.api.types as pd_types

logger = logging.getLogger(__name__)
setup(logger)


def clean_pipeline(df: pd.DataFrame) -> pd.DataFrame:
    """
    Main orchestration function for the cleaning pipeline.

    Applies the following steps in order:
    1. Standardize column names (e.g., lowercase, snake_case)
    2. Remove exact duplicate rows
    3. Coerce column types (booleans, dates, IDs, integers)
    4. Handle missing values (drop columns with >50% missing, impute others)

    Parameters:
        df (pd.DataFrame): The input DataFrame to be cleaned.

    Returns:
        pd.DataFrame: The cleaned DataFrame.
    """
    df = standardize_column_names(df)
    df = remove_duplicates(df)
    df = coerce_data_types(df)
    df = handle_missing_values(df)
    return df


def standardize_column_names(df: pd.DataFrame) -> pd.DataFrame:
    """
    Standardizes DataFrame column names by:
    - Removing leading/trailing whitespace
    - Converting to lowercase
    - Replacing spaces/hyphens with underscores
    - Removing all non-alphanumeric characters (except underscores)

    Also removes a leading 'Unnamed' column if generated by Pandas.

    Parameters:
        df (pd.DataFrame): The input DataFrame.

    Returns:
        pd.DataFrame: DataFrame with standardized column names.
    """
    logger.info("Standardizing column names")

    if df.iloc[:, 0].name.startswith("Unnamed"):
        logger.info("Removing pandas index column")
        df = df.iloc[:, 1:]
        logger.info("Index column removed")

    columns_nr = df.shape[1]
    logger.info(f"Found {columns_nr} columns")
    logger.info(
        "replacing whitespaces with '_', lowering case, and removing invalid characters"
    )
    logger.info(f"Columns before operation {df.columns.to_list()}")
    df.columns = (
        df.columns.str.strip()
        .str.lower()
        .str.replace(r"[ -]", "_", regex=True)
        .str.replace(r"[^\w]", "", regex=True)
    )
    logger.info(f"Columns after operation {df.columns.to_list()}")
    logger.info("Finished standardizing columns")
    return df


def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:
    """
    Function that removes duplicate rows, but also removes a row by following the procedure below:
    (Rest is deprecated)
    1) Identify 'id' column names (regex validation)
    2) Check for duplication, excluding the 'id' columns in the subset
    3) Remove duplication, keep in the first occurrence.
    In any case we are removing duplicates with the 'keep-first' strategy
    """
    logger.info("Removing duplicate rows")
    logger.info(f"{df.shape[0]} rows before operation")
    logger.info("Removing...")
    try:
        no_dup_df = df.drop_duplicates(keep="first")
    except TypeError:
        logger.error(
            "Dataset contains unhashable type columns, cannot remove rows based on full row duplication."
        )
        no_dup_df = df

    logger.info(f"{df.shape[0] - no_dup_df.shape[0]} rows removed")
    logger.info(f"{no_dup_df.shape[0]} rows remaining.")
    logger.info(f"Finished removing duplicate rows")
    return no_dup_df


def coerce_data_types(df: pd.DataFrame) -> pd.DataFrame:
    """
    Processes column series, and applies _coerce_series to
    each resulting in a dataframe with the correct data types

    Parameters:
        df (pd.DataFrame): The source Dataframe

    Returns:
        df (pd.DataFrame): The source dataframe with updated
        column series
    """
    logger.info("Handling data types")
    for col_name in df.columns:
        logger.info(f"Processing column {col_name}")
        col_series = df[col_name]
        df[col_name] = _coerce_series(col_series)
    logger.info("Finished handling data types")
    return df


def handle_missing_values(
    df: pd.DataFrame, drop_thres: float = 0.5
) -> pd.DataFrame:
    """
    Handles missing values in a DataFrame using a two-step strategy:

    1. If a column has a percentage of missing values >= `drop_thres` (default 50%),
       it is dropped entirely.
    2. Otherwise, missing values are imputed using `_impute()`, which:
        - Imputes numerics with the median (or another strategy)
        - Skips categorical and identifier-like columns

    Parameters:
        df (pd.DataFrame): The input DataFrame with potential missing values.
        drop_thres (float, optional): Threshold (as a proportion) for dropping a column
                                       based on missing value percentage. Defaults to 0.5.

    Returns:
        pd.DataFrame: The DataFrame with missing values either dropped or imputed.
    """
    logger.info("Handling missing Values")
    for column in df.columns:
        missing_values_prc = df[column].isnull().mean() * 100
        if missing_values_prc == 0:
            logger.info(
                f"{df[column].name} column has none of its values missing, skipping"
            )
            continue
        logger.info(
            f"{df[column].name} column has {round(missing_values_prc)}% of its values missing"
        )
        if missing_values_prc >= 50:
            logger.info(f"Dropping column {df[column].name}")
            df = df.drop(column, axis=1)
        else:
            df[column] = _impute(df[column])
    logger.info("Finished Handling Missing Values")
    return df


def _coerce_series(col_series: pd.Series) -> pd.Series:
    """
    Function that fixes a column series datatypes based on the following rules:
    - If a column contains only 'yes' - 'no' or 'true' - 'false' both
    case insensitive, the column will change its type to 'bool'
    - If a column contains the word 'id' in its name (regex validation),
    it will be converted to object.

    Parameters:
        col_series (pd.Series): The source column series

    Returns:
        Either col_series as is, or a copy of it with updated it type.
    """
    col_name = col_series.name  # Extracting column name from the series
    col_dtype = col_series.dtype.name
    logger.info(f"Initial dtype is {col_dtype}")

    if _is_float_that_can_be_int(col_series):
        col_series = col_series.astype("Int64")
        logger.info(f"Changed {col_name}'s dtype to Int64")

    if _is_id_column(col_series):
        col_series = col_series.astype("string")
        logger.info(f"Changed {col_name} from numeric to string")

    elif _is_binary_string(col_series):
        logger.info("It is a binary string")
        col_series = _validate_binary_col(col_series)

    elif _is_numeric_binary(col_series):
        col_series = col_series.astype("boolean")
        logger.info(f"Changed {col_name}'s dtype to boolean")

    col_series = _convert_dates(col_series)
    if col_dtype == col_series.dtype.name:
        logger.info("No change needed")
    else:
        logger.info(f"Final dtype is {col_series.dtype.name}")
    return col_series


def _is_float_that_can_be_int(col_series: pd.Series) -> bool:
    return (
        pd_types.is_float_dtype(col_series)
        and col_series.apply(
            lambda x: pd.isna(x) or float(x).is_integer()
        ).all()
    )


def _is_binary_string(col_series: pd.Series) -> bool:
    """ """
    return (
        pd_types.is_object_dtype(col_series)
        and col_series.dropna().apply(str).str.lower().nunique() == 2
    )


def _is_numeric_binary(col_series: pd.Series) -> bool:
    return pd_types.is_integer_dtype(col_series) and set(
        col_series.dropna().unique()
    ).issubset({0, 1})


def _is_id_column(col_series: pd.Series) -> bool:
    col_name = col_series.name
    return pd_types.is_numeric_dtype(col_series) and re.search(
        r"((?:(?<=_)|^)id(?=_))|.*id$", col_name, re.IGNORECASE
    )


def _convert_dates(col_series: pd.Series) -> pd.Series:
    if pd_types.is_object_dtype(col_series) or pd_types.is_string_dtype(
        col_series
    ):
        converted = pd.to_datetime(
            col_series, errors="coerce", utc=True
        )
        if converted.notna().mean() > 0.8:
            logger.info(f"Converted {col_series.name} to datetime")
            return converted
    return col_series


def _impute(col_series: pd.Series, nmode="median") -> pd.Series:
    """
    Using nmode to impute the values of the provided column
    In case of invalid input the default nmode will be used.

    Note:
        Only addresses non-categorical, non-id columns
    """

    if (
        pd_types.is_numeric_dtype(col_series)
        and not _is_categorical(col_series)
        and not _is_id_column(col_series)
    ):
        logger.info(
            f"Initiating imputation on {col_series.name} of dtype {col_series.dtype.name}"
        )
        logger.info(col_series.dtype.name)
        if nmode == "median":
            logger.info("Performing imputation with 'median'")
            col_series = col_series.fillna(col_series.median())
        elif nmode == "mean":
            logger.info("Performing imputation with 'mean'")
            col_series = col_series.fillna(col_series.mean())
        else:
            default_nmode = _impute.__defaults__[0]
            logger.info(f"Performing imputation with '{default_nmode}'")
            func = getattr(pd.Series, default_nmode)
            col_series = col_series.fillna(func(col_series))

    else:
        logger.info(
            f"{col_series.name} column, unsuitable for imputation, skipping."
        )

    return col_series


def _validate_binary_col(col_series: pd.Series) -> pd.Series:
    """
    Detect boolean-like columns and convert them to columns of bool dtype, otherwise convert them to string
    """
    # Check if all non-null rows are 'true' or 'false'
    if all(
        map(
            lambda x: (
                True
                if (
                    not x
                    or str(x).lower() == "true"
                    or str(x).lower() == "false"
                )
                and not re.match(r"\s*$", str(x))
                else False
            ),
            col_series,
        )
    ):
        # if they are convert them to boolean
        col_series = col_series.map(_true_false_to_bool).astype(
            "boolean"
        )
        logger.info(f"Changed {col_series.name}'s dtype to boolean")
    # Check if all non-null rows are 'yes' or 'no'
    elif all(
        map(
            lambda x: (
                True
                if (
                    not x
                    or str(x).lower() == "yes"
                    or str(x).lower() == "no"
                )
                and not re.match(r"\s*$", str(x))
                else False
            ),
            col_series,
        )
    ):
        # if they are convert them to boolean
        col_series = col_series.map(_yes_no_to_bool).astype("boolean")
        logger.info(f"Changed {col_series.name}'s dtype to boolean")
    else:
        col_series = col_series.astype("string")

    return col_series


def _is_categorical(col_series: pd.Series) -> bool:
    return col_series.nunique(dropna=True) < 13


def _yes_no_to_bool(x):
    "Converts a value to True if it is 'yes', False otherwise"
    if x is None:
        return pd.NA
    else:
        func = lambda x: True if x.lower() == "yes" else False
        return func(x)


def _true_false_to_bool(x):
    "Converts a value to True if it is 'true', False otherwise"
    if x is None:
        return pd.NA
    else:
        func = lambda x: True if x.lower() == "true" else False
        return func(x)
